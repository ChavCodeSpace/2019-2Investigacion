\documentclass{article}
\usepackage[utf8]{inputenc}

\title{Como la Crisis de Fundamentos Derivo en la Computación Moderna}
\author{Chrsitian Gallego Chaverra}
\date{Marzo 2020}

\usepackage{natbib}
\usepackage{graphicx}

\begin{document}

\maketitle

\section{Introducción}
Durante toda la historia de la humanidad el desarrollo matemático a ido de la mano con el desarrollo computacional. Cada avance en uno es seguido inmediatamente por un avance en el otro. Cuando la humanidad desarrolló el concepto del sistema de conteo en base diez, el ábaco fue una herramienta para hacerlo más fácil. Cuando las computadoras electrónicas fueron construidas para resolver ecuaciones complejas, campos como la dinámica de fluidos, teoría de los números, y la física química floreció.
Es así como hemos llegado a este punto en la historia, donde encontramos que la tecnología hace parte de casi todas las tareas diarias de casi todo el mundo. La computación ha encontrado aplicaciones en casi todos los campos de estudio y la humanidad cada vez esta mas acostumbrada con la tecnología. Encontramos desde casas a ciudades inteligentes, múltiples aparatos tecnológicos que pueden estar en red para un mismo usuario, lo que impulsa las redes de área personal (PAN) \citep{pan}.
Tanto a sido el crecimiento de la computación, que hoy en día la humanidad busca dotar de inteligencia a las maquinas y así hacerlas autosuficientes y pensantes para desarrollar diferentes problemas; a esto se le conoce como aprendizaje de maquina o machine learning, que va de la mano con la inteligencia artificial, otro de los grandes proyectos actuales de la humanidad que ha demostrado grandes resultados en donde se ha aplicado. La inteligencia artificial podría considerarse el resultado de la computación moderna y el siguiente escalón para una nueva era de computación y tecnología para la humanidad, pues, la IA ha facilitado muchas tareas y ayudado a grandes proyectos que antes no tenían forma de solución, impulsando a compañías y empresas tecnológicas a crecer cada vez mas. \citep{ia}
  
\section{Desarrollo}
Se podría decir que todo empieza desde el desarrollo del ábaco, que se considera la primera maquina que podía realizar cálculos matemáticos de la historia, pues realizaba operaciones como sumas, restas, multiplicaciones, divisiones, extraer la raíz cuadrada o la raíz cúbica.\citep{abaco}).
Después de este invento, fue hasta el siglo XVII en el año de 1617 que el matemático escoces John Napier \citep{neper}, famoso por su invención de los logaritmos, desarrolló un sistema para realizar operaciones aritméticas manipulando barras, a las que llamó "huesos" ya que estaban construidas con material de hueso o marfil, y en los que estaban plasmados los dígitos. Dada su naturaleza, se llamó al sistema "huesos de Napier" (ábaco neperiano). Los huesos de Napier tuvieron una fuerte influencia en el desarrollo de la regla deslizante, por parte del matemático ingles William Oughtred, cinco años más tarde, y las máquinas calculadoras subsecuentes, que contaron con logaritmos.
Después, en 1645  Blaise Pascal inventa la pascalina. Con esta máquina, los datos se representaban mediante las posiciones de los engranajes. La pascalina es una de las primeras calculadoras mecánicas, que funcionaba a base de ruedas de diez dientes en las que cada uno de los dientes representaba un dígito del 0 al 9. \citep{pascalina}
Gracias a este invento, el  matemático alemán Gottfried Leibniz inventa la primera calculadora de propósito general en 1673.
Desde aquí se iba evidenciando como la humanidad iba empezando a juntar conocimientos y creando nuevas maneras de pensar. Un periodo de tiempo critico para la evolución de la computación es el comprendido entre 1823 y 1936, pues durante este tiempo, muchas de las culturas del mundo fueron avanzando desde sociedades basadas en la agricultura a sociedades basadas industrialmente. Con estos cambios vinieron los avances matemáticos y en ingeniería los cuales hicieron posible máquinas electrónicas que pueden resolver argumentos lógicos complejos. 
En 1854 aparece el lógico inglés George Boole publica su Álgebra de Boole. El sistema de Boole redujo a argumentos lógicos las permutaciones de tres operadores básicos algebraicos: y, o, y no. A causa del desarrollo del álgebra de Boole, Boole es considerado por muchos como el padre de la teoría de la informática. \citep{boole}. En 1869 la primera maquina que usa el algebra booleana es inventada por William Stanley Jevons, llamada piano lógico, usó un alfabeto de cuatro términos lógicos para resolver silogismos complicados.
Después de estos sucesos, en 1906 aparece el tubo vació, inventado por el estadounidense Lee De Forest. El tubo al vacío encontraría uso en varias generaciones tempranas de 5 computadoras, a comienzos de 1930. En 1914 el español Leonardo Torres Quevedo \citep{queved}, establece las bases de la automática, plantea la problemática de la inteligencia artificial (sin introducir esta denominación) y la aritmética en coma flotante en sus Ensayos sobre automática. 
La segunda guerra mundial los estudios en la computación fueron impulsados y el desarrollo empezó su nido, valiéndose de la tecnología eléctrica que permitió un rápido avance, pues en 1939 la primera computadora electrónica digital se desarrolló en la Universidad del Estado de Iowa por Dr. John V. Atanasoff y Clifford Baya. El prototipo, llamó el Atanasoff Berry Computer (ABC) \citep{abc}, fue la primera máquina en hacer uso de tubos al vacío como los circuitos de la lógica y en 1941 la primera controladora para computadora para propósito general usada se construyó por Konrad Zuse y Helmut Schreyer. El "Z-3," como se llamó, usaba retardos electromagnéticos y era programada usando películas agujereadas.
En 1947 se inventó la primera resistencia de traslado, (transistor) en Laboratorios Bell por John Bardeen, Walter H. Brattain, y William Shockley. Los diseñadores recibieron el Premio Nobel en 1956 por su trabajo. El transistor es un componente pequeño que deja la regulación del flujo eléctrico presente. El uso de transistores como interruptores habilitaron computadoras llegar a ser mucho más pequeño y secuencialmente llevó al desarrollo de la tecnología de la "microelectrónica".
En 1949 la primera memoria fue desarrollada por Jay Forrester.\citep{jay} Empezando en 1953, la memoria, que constó de una reja de anillos magnéticos en alambre interconectados, reemplazó los no confiables tubos al vacío como la forma predominante de memoria por los próximos diez años

\section{Conclusión}
Después de la invención de la memoria se pudo llegar al computador convencional y de ahí se pudo crecer hasta donde estamos ahora. La crisis de los fundamentos fue necesaria para la evolución computacional, pues cada vez que la humanidad llega a un callejón sin salida, lo supera mirando hacia atrás en la historia y viendo como se puede avanzar a partir de lo que ya se tiene o buscando nuevas formas y soluciones, también basándose en la historia, pues bien dice el conocido refrán: "Quien no conoce la historia esta condenado a repetirla".
Igual esto no se detiene aquí. Mientras avanza el tiempo la humanidad se tendrá que enfrentar a retos computacionales cada vez mayores para poder seguir evolucionando. 

\bibliographystyle{plain}
\bibliography{references}
\end{document}
